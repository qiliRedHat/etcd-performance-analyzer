"""
etcd Analyzer Performance Utility Module
Common utility functions for etcd performance analysis
"""

import asyncio
import logging
import uuid
from typing import Dict, List, Any, Optional
from datetime import datetime
import pytz


class etcdAnalyzerUtility:
    """Utility class for etcd performance analysis operations"""
    
    def __init__(self):
        self.logger = logging.getLogger(__name__)
        self.timezone = pytz.UTC
    
    def generate_test_id(self) -> str:
        """Generate a unique test ID for analysis sessions"""
        return str(uuid.uuid4())
    
    def format_metric_value(self, value: float, unit: str) -> Dict[str, Any]:
        """Format metric value with appropriate unit and readability"""
        if value is None:
            return {"raw": None, "formatted": "N/A"}
        
        formatted_value = {
            "raw": value,
            "formatted": "N/A"
        }
        
        try:
            if unit.lower() in ['milliseconds', 'ms']:
                if value >= 1000:
                    formatted_value["formatted"] = f"{value/1000:.3f}s"
                else:
                    formatted_value["formatted"] = f"{value:.3f}ms"
            elif unit.lower() in ['seconds', 's']:
                if value >= 60:
                    minutes = value / 60
                    if minutes >= 60:
                        hours = minutes / 60
                        formatted_value["formatted"] = f"{hours:.2f}h"
                    else:
                        formatted_value["formatted"] = f"{minutes:.2f}m"
                else:
                    formatted_value["formatted"] = f"{value:.3f}s"
            elif unit.lower() in ['bytes']:
                if value >= 1024**3:
                    formatted_value["formatted"] = f"{value/(1024**3):.2f}GB"
                elif value >= 1024**2:
                    formatted_value["formatted"] = f"{value/(1024**2):.2f}MB"
                elif value >= 1024:
                    formatted_value["formatted"] = f"{value/1024:.2f}KB"
                else:
                    formatted_value["formatted"] = f"{value:.0f}B"
            elif unit.lower() in ['bytes_per_second']:
                if value >= 1024**3:
                    formatted_value["formatted"] = f"{value/(1024**3):.2f}GB/s"
                elif value >= 1024**2:
                    formatted_value["formatted"] = f"{value/(1024**2):.2f}MB/s"
                elif value >= 1024:
                    formatted_value["formatted"] = f"{value/1024:.2f}KB/s"
                else:
                    formatted_value["formatted"] = f"{value:.0f}B/s"
            elif unit.lower() in ['operations_per_second', 'ops/s']:
                formatted_value["formatted"] = f"{value:.3f}/s"
            elif unit.lower() in ['percent', '%']:
                formatted_value["formatted"] = f"{value:.2f}%"
            elif unit.lower() in ['count', 'total']:
                formatted_value["formatted"] = f"{int(value):,}"
            else:
                # Default formatting for unknown units
                if value >= 1000000:
                    formatted_value["formatted"] = f"{value/1000000:.2f}M {unit}"
                elif value >= 1000:
                    formatted_value["formatted"] = f"{value/1000:.2f}K {unit}"
                else:
                    formatted_value["formatted"] = f"{value:.3f} {unit}"
        
        except Exception as e:
            self.logger.warning(f"Error formatting value {value} with unit {unit}: {e}")
            formatted_value["formatted"] = f"{value} {unit}"
        
        return formatted_value
    
    def extract_pod_metrics(self, metric_data: Dict[str, Any], metric_name: str) -> List[Dict[str, Any]]:
        """Extract pod-level metrics from collected data"""
        pod_metrics = []
        
        try:
            if metric_data.get('status') == 'success':
                # Handle different data structures
                pods_data = None
                
                if 'pod_metrics' in metric_data:
                    pods_data = metric_data['pod_metrics']
                elif 'pods' in metric_data:
                    pods_data = metric_data['pods']
                elif 'data' in metric_data and 'pods' in metric_data['data']:
                    pods_data = metric_data['data']['pods']
                
                if pods_data:
                    for pod_name, pod_stats in pods_data.items():
                        pod_metric = {
                            "metric_name": metric_name,
                            "pod_name": pod_name,
                            "avg": pod_stats.get('avg'),
                            "max": pod_stats.get('max'),
                            "unit": metric_data.get('unit', 'unknown')
                        }
                        
                        # Add node information if available
                        if 'node' in pod_stats:
                            pod_metric["node"] = pod_stats['node']
                        
                        pod_metrics.append(pod_metric)
        
        except Exception as e:
            self.logger.error(f"Error extracting pod metrics for {metric_name}: {e}")
        
        return pod_metrics
    
    def extract_node_metrics(self, metric_data: Dict[str, Any], metric_name: str) -> List[Dict[str, Any]]:
        """Extract node-level metrics from collected data"""
        node_metrics = []
        
        try:
            if metric_data.get('status') == 'success':
                nodes_data = None
                
                if 'nodes' in metric_data:
                    nodes_data = metric_data['nodes']
                elif 'data' in metric_data and 'nodes' in metric_data['data']:
                    nodes_data = metric_data['data']['nodes']
                
                if nodes_data:
                    for node_name, node_stats in nodes_data.items():
                        node_metric = {
                            "metric_name": metric_name,
                            "node_name": node_name,
                            "avg": node_stats.get('avg'),
                            "max": node_stats.get('max'),
                            "unit": metric_data.get('unit', 'unknown')
                        }
                        
                        # Add device information if available
                        if 'devices' in node_stats:
                            node_metric["devices"] = node_stats['devices']
                        elif 'device_count' in node_stats:
                            node_metric["device_count"] = node_stats['device_count']
                        
                        node_metrics.append(node_metric)
        
        except Exception as e:
            self.logger.error(f"Error extracting node metrics for {metric_name}: {e}")
        
        return node_metrics
    
    def extract_cluster_metrics(self, metric_data: Dict[str, Any], metric_name: str, test_id: str) -> Dict[str, Any]:
        """Extract cluster-level metrics from collected data"""
        cluster_metric = {
            "metric_name": metric_name,
            "test_id": test_id,
            "avg": None,
            "max": None,
            "unit": "unknown",
            "query": None
        }
        
        try:
            if metric_data.get('status') == 'success':
                cluster_metric["avg"] = metric_data.get('avg')
                cluster_metric["max"] = metric_data.get('max')
                cluster_metric["unit"] = metric_data.get('unit', 'unknown')
                cluster_metric["query"] = metric_data.get('query')
        
        except Exception as e:
            self.logger.error(f"Error extracting cluster metrics for {metric_name}: {e}")
        
        return cluster_metric
    
    def analyze_latency_patterns(self, metrics_data: Dict[str, Any]) -> Dict[str, Any]:
        """Analyze latency patterns and identify potential bottlenecks"""
        analysis = {
            "potential_bottlenecks": [],
            "latency_analysis": {},
            "recommendations": []
        }
        
        try:
            # Analyze WAL fsync latency
            if 'wal_fsync_data' in metrics_data:
                wal_metrics = metrics_data['wal_fsync_data']
                p99_data = next((m for m in wal_metrics if 'p99' in m.get('metric_name', '')), None)
                
                if p99_data and p99_data.get('avg'):
                    avg_latency = p99_data['avg']
                    max_latency = p99_data.get('max', avg_latency)
                    
                    analysis['latency_analysis']['wal_fsync_p99'] = {
                        "avg_ms": avg_latency * 1000 if avg_latency < 1 else avg_latency,
                        "max_ms": max_latency * 1000 if max_latency < 1 else max_latency,
                        "status": self._assess_latency_status(avg_latency, 0.01, 0.05, 0.1)  # 10ms, 50ms, 100ms thresholds
                    }
                    
                    if avg_latency > 0.1:  # > 100ms
                        analysis['potential_bottlenecks'].append({
                            "type": "disk_io",
                            "description": "High WAL fsync latency indicates disk I/O bottleneck",
                            "severity": "high",
                            "metric": "wal_fsync_p99",
                            "value": f"{avg_latency:.3f}s"
                        })
                    elif avg_latency > 0.05:  # > 50ms
                        analysis['potential_bottlenecks'].append({
                            "type": "disk_io",
                            "description": "Elevated WAL fsync latency may indicate disk performance issues",
                            "severity": "medium",
                            "metric": "wal_fsync_p99",
                            "value": f"{avg_latency:.3f}s"
                        })
            
            # Analyze backend commit latency
            if 'backend_commit_data' in metrics_data:
                commit_metrics = metrics_data['backend_commit_data']
                p99_commit = next((m for m in commit_metrics if 'p99' in m.get('metric_name', '')), None)
                
                if p99_commit and p99_commit.get('avg'):
                    commit_latency = p99_commit['avg']
                    analysis['latency_analysis']['backend_commit_p99'] = {
                        "avg_ms": commit_latency * 1000 if commit_latency < 1 else commit_latency,
                        "status": self._assess_latency_status(commit_latency, 0.005, 0.02, 0.05)  # 5ms, 20ms, 50ms
                    }
                    
                    if commit_latency > 0.05:  # > 50ms
                        analysis['potential_bottlenecks'].append({
                            "type": "backend_commit",
                            "description": "High backend commit latency indicates database performance issues",
                            "severity": "high",
                            "metric": "backend_commit_p99",
                            "value": f"{commit_latency:.3f}s"
                        })
            
            # Analyze network latency
            if 'network_data' in metrics_data:
                network_metrics = metrics_data['network_data']
                peer_latency = next((m for m in network_metrics.get('pod_metrics', []) 
                                   if 'peer2peer_latency' in m.get('metric_name', '')), None)
                
                if peer_latency and peer_latency.get('avg'):
                    p2p_latency = peer_latency['avg']
                    analysis['latency_analysis']['peer_to_peer'] = {
                        "avg_ms": p2p_latency * 1000 if p2p_latency < 1 else p2p_latency,
                        "status": self._assess_latency_status(p2p_latency, 0.01, 0.05, 0.1)
                    }
                    
                    if p2p_latency > 0.1:  # > 100ms
                        analysis['potential_bottlenecks'].append({
                            "type": "network",
                            "description": "High peer-to-peer latency indicates network performance issues",
                            "severity": "high",
                            "metric": "peer2peer_latency_p99",
                            "value": f"{p2p_latency:.3f}s"
                        })
            
            # Generate recommendations
            analysis['recommendations'] = self._generate_recommendations(analysis['potential_bottlenecks'])
        
        except Exception as e:
            self.logger.error(f"Error analyzing latency patterns: {e}")
            analysis['error'] = str(e)
        
        return analysis
    
    def _assess_latency_status(self, value: float, good_threshold: float, warning_threshold: float, critical_threshold: float) -> str:
        """Assess latency status based on thresholds"""
        if value <= good_threshold:
            return "excellent"
        elif value <= warning_threshold:
            return "good"
        elif value <= critical_threshold:
            return "warning"
        else:
            return "critical"
    
    def _generate_recommendations(self, bottlenecks: List[Dict[str, Any]]) -> List[str]:
        """Generate recommendations based on identified bottlenecks"""
        recommendations = []
        
        disk_issues = [b for b in bottlenecks if b['type'] == 'disk_io']
        network_issues = [b for b in bottlenecks if b['type'] == 'network']
        backend_issues = [b for b in bottlenecks if b['type'] == 'backend_commit']
        
        if disk_issues:
            recommendations.extend([
                "Check disk I/O performance and consider faster storage (NVMe SSD)",
                "Verify etcd data directory is on dedicated disk with sufficient IOPS",
                "Monitor disk utilization and consider storage optimization"
            ])
        
        if network_issues:
            recommendations.extend([
                "Check network connectivity between etcd cluster members",
                "Verify network bandwidth and latency between nodes",
                "Consider network topology optimization for etcd cluster"
            ])
        
        if backend_issues:
            recommendations.extend([
                "Consider etcd database compaction and defragmentation",
                "Monitor database size and optimize key-value operations",
                "Review etcd configuration for performance tuning"
            ])
        
        if not bottlenecks:
            recommendations.append("No significant performance bottlenecks detected")
        
        return recommendations
    
    def create_performance_summary(self, all_metrics: Dict[str, Any]) -> Dict[str, Any]:
        """Create a comprehensive performance summary"""
        summary = {
            "timestamp": datetime.now(self.timezone).isoformat(),
            "total_metrics_collected": 0,
            "categories": {
                "general_info": {"count": 0, "status": "unknown"},
                "wal_fsync": {"count": 0, "status": "unknown"},
                "disk_io": {"count": 0, "status": "unknown"},
                "network_io": {"count": 0, "status": "unknown"},
                "backend_commit": {"count": 0, "status": "unknown"},
                "compact_defrag": {"count": 0, "status": "unknown"}
            },
            "performance_indicators": {},
            "overall_health": "unknown"
        }
        
        try:
            # Count metrics by category
            category_alias = {
                'network': 'network_io'
            }
            for category, data in all_metrics.items():
                raw_key = category.replace('_data', '')
                key = category_alias.get(raw_key, raw_key)
                if key not in summary['categories']:
                    # Unknown category; skip counting but continue
                    self.logger.debug(f"Unknown category '{key}' in summary counting; available: {list(summary['categories'].keys())}")
                    continue
                if isinstance(data, list):
                    summary['categories'][key]['count'] = len(data)
                    summary['total_metrics_collected'] += len(data)
                elif isinstance(data, dict) and 'pod_metrics' in data:
                    count = len(data['pod_metrics'])
                    summary['categories'][key]['count'] = count
                    summary['total_metrics_collected'] += count
            
            # Assess overall health based on latency analysis
            if 'latency_analysis' in all_metrics:
                latency_data = all_metrics['latency_analysis']
                health_scores = []
                
                for metric, analysis in latency_data.get('latency_analysis', {}).items():
                    status = analysis.get('status', 'unknown')
                    if status == 'excellent':
                        health_scores.append(4)
                    elif status == 'good':
                        health_scores.append(3)
                    elif status == 'warning':
                        health_scores.append(2)
                    elif status == 'critical':
                        health_scores.append(1)
                
                if health_scores:
                    avg_score = sum(health_scores) / len(health_scores)
                    if avg_score >= 3.5:
                        summary['overall_health'] = 'excellent'
                    elif avg_score >= 2.5:
                        summary['overall_health'] = 'good'
                    elif avg_score >= 1.5:
                        summary['overall_health'] = 'warning'
                    else:
                        summary['overall_health'] = 'critical'
        
        except Exception as e:
            self.logger.error(f"Error creating performance summary: {e}")
            summary['error'] = str(e)
        
        return summary
    
    def format_timestamp(self, timestamp: Optional[str] = None) -> str:
        """Format timestamp in UTC"""
        if timestamp:
            return timestamp
        return datetime.now(self.timezone).isoformat()
    
    def safe_extract_value(self, data: Dict[str, Any], keys: List[str], default: Any = None) -> Any:
        """Safely extract nested values from dictionary"""
        try:
            current = data
            for key in keys:
                current = current[key]
            return current
        except (KeyError, TypeError):
            return default